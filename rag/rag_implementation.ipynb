{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SQuAD v2 dataset\n",
    "dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Select a subset of the dataset\n",
    "contexts = [example[\"context\"] for example in train_dataset.select(range(1000))]\n",
    "questions = [example[\"question\"] for example in train_dataset.select(range(1000))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get embeddings\n",
    "def embed_texts(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Embed the contexts\n",
    "context_embeddings = embed_texts(contexts).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: (1000, 384)\n"
     ]
    }
   ],
   "source": [
    "# Print dimensions for debugging\n",
    "print(\"Embedding dimension:\", context_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize embeddings for cosine similarity\n",
    "context_embeddings /= np.linalg.norm(context_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 250690.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataset with required columns\n",
    "data_dict = {\n",
    "    \"title\": [\"\"] * len(contexts),  # Dummy titles as SQuAD does not provide titles\n",
    "    \"text\": contexts,\n",
    "    \"embeddings\": context_embeddings.tolist()  # Convert to list for serialization\n",
    "}\n",
    "new_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Save the new dataset to disk\n",
    "dataset_path = \"squad_v2_embeddings_v1\"\n",
    "new_dataset.save_to_disk(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Create and save the FAISS index\n",
    "distance_type = \"Cosine\"\n",
    "index_path = \"faiss_index_v1.faiss\"\n",
    "dimension = context_embeddings.shape[1]\n",
    "\n",
    "# Create the FAISS index\n",
    "if distance_type == \"L2\":\n",
    "    faiss_index = faiss.IndexFlatL2(dimension)\n",
    "elif distance_type == \"Cosine\":\n",
    "    faiss_index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "# Add the context embeddings to the index\n",
    "faiss_index.add(context_embeddings)\n",
    "faiss.write_index(faiss_index, index_path)\n",
    "\n",
    "print(faiss_index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Some weights of the model checkpoint at facebook/rag-sequence-nq were not used when initializing RagSequenceForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RagSequenceForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagSequenceForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# Load RAG components\n",
    "generator_model_name = \"facebook/rag-sequence-nq\"\n",
    "rag_tokenizer = RagTokenizer.from_pretrained(generator_model_name)\n",
    "rag_model = RagSequenceForGeneration.from_pretrained(generator_model_name)\n",
    "\n",
    "# Initialize RAG retriever\n",
    "rag_retriever = RagRetriever.from_pretrained(\n",
    "    generator_model_name,\n",
    "    index_name=\"custom\",\n",
    "    passages_path=dataset_path,\n",
    "    index_path=index_path,\n",
    "    use_dummy_dataset=False\n",
    ")\n",
    "\n",
    "# Associate the retriever with the RAG model\n",
    "rag_model.set_retriever(rag_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 384)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rag_tokenizer\u001b[38;5;241m.\u001b[39mdecode(result[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Example question\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the capital of France?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m rag_tokenizer(question, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrag_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rag_tokenizer\u001b[38;5;241m.\u001b[39mdecode(result[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/transformers/models/rag/modeling_rag.py:984\u001b[0m, in \u001b[0;36mRagSequenceForGeneration.generate\u001b[0;34m(self, input_ids, attention_mask, context_input_ids, context_attention_mask, doc_scores, do_deduplication, num_return_sequences, num_beams, n_docs, **model_kwargs)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    983\u001b[0m     question_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestion_encoder(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 984\u001b[0m     context_input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;66;03m# set to correct device\u001b[39;00m\n\u001b[1;32m    993\u001b[0m     context_input_ids \u001b[38;5;241m=\u001b[39m context_input_ids\u001b[38;5;241m.\u001b[39mto(input_ids)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/transformers/models/rag/retrieval_rag.py:629\u001b[0m, in \u001b[0;36mRagRetriever.__call__\u001b[0;34m(self, question_input_ids, question_hidden_states, prefix, n_docs, return_tensors)\u001b[0m\n\u001b[1;32m    627\u001b[0m n_docs \u001b[38;5;241m=\u001b[39m n_docs \u001b[38;5;28;01mif\u001b[39;00m n_docs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_docs\n\u001b[1;32m    628\u001b[0m prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mprefix\n\u001b[0;32m--> 629\u001b[0m retrieved_doc_embeds, doc_ids, docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m input_strings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestion_encoder_tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(question_input_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    632\u001b[0m context_input_ids, context_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess_docs(\n\u001b[1;32m    633\u001b[0m     docs, input_strings, prefix, n_docs, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors\n\u001b[1;32m    634\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/transformers/models/rag/retrieval_rag.py:578\u001b[0m, in \u001b[0;36mRagRetriever.retrieve\u001b[0;34m(self, question_hidden_states, n_docs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, question_hidden_states: np\u001b[38;5;241m.\u001b[39mndarray, n_docs: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, List[\u001b[38;5;28mdict\u001b[39m]]:\n\u001b[1;32m    560\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;124;03m    Retrieves documents for specified `question_hidden_states`.\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m        - **doc_dicts** (`List[dict]`): The `retrieved_doc_embeds` examples per query.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m     doc_ids, retrieved_doc_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_doc_embeds, doc_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_doc_dicts(doc_ids)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/transformers/models/rag/retrieval_rag.py:548\u001b[0m, in \u001b[0;36mRagRetriever._main_retrieve\u001b[0;34m(self, question_hidden_states, n_docs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question_hidden_states \u001b[38;5;129;01min\u001b[39;00m question_hidden_states_batched:\n\u001b[1;32m    547\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 548\u001b[0m     ids, vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_top_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex search time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec, batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion_hidden_states\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m     ids_batched\u001b[38;5;241m.\u001b[39mextend(ids)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/transformers/models/rag/retrieval_rag.py:230\u001b[0m, in \u001b[0;36mHFIndexBase.get_top_docs\u001b[0;34m(self, question_hidden_states, n_docs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_top_docs\u001b[39m(\u001b[38;5;28mself\u001b[39m, question_hidden_states: np\u001b[38;5;241m.\u001b[39mndarray, n_docs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m--> 230\u001b[0m     _, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     docs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m indices \u001b[38;5;129;01min\u001b[39;00m ids]\n\u001b[1;32m    232\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m [doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/datasets/search.py:733\u001b[0m, in \u001b[0;36mIndexableMixin.search_batch\u001b[0;34m(self, index_name, queries, k, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find the nearest examples indices in the dataset to the query.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \n\u001b[1;32m    718\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;124;03m        - **total_indices** (`List[List[int]]`): the indices of the retrieved examples per query\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_index_is_initialized(index_name)\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indexes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/datasets/search.py:384\u001b[0m, in \u001b[0;36mFaissIndex.search_batch\u001b[0;34m(self, queries, k, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m queries\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous:\n\u001b[1;32m    383\u001b[0m     queries \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(queries, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 384\u001b[0m scores, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfaiss_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchedSearchResults(scores, indices\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/faiss/class_wrappers.py:329\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[0;34m(self, x, k, params, D, I)\u001b[0m\n\u001b[1;32m    327\u001b[0m n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    328\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 329\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m D \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_answer(question):\n",
    "    inputs = rag_tokenizer(question, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        result = rag_model.generate(input_ids=inputs[\"input_ids\"], num_return_sequences=1)\n",
    "    return rag_tokenizer.decode(result[0], skip_special_tokens=True)\n",
    "\n",
    "# Example question\n",
    "print(generate_answer(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 384\n",
    "faiss_index = faiss.IndexFlatIP(dimension)\n",
    "assert faiss_index.d == dimension, f\"Expected dimension {dimension}, but got {faiss_index.d}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 8.92k/8.92k [00:00<00:00, 6.66MB/s]\n",
      "Downloading data: 100%|██████████| 16.4M/16.4M [00:01<00:00, 10.2MB/s]\n",
      "Downloading data: 100%|██████████| 1.35M/1.35M [00:00<00:00, 2.44MB/s]\n",
      "Generating train split: 100%|██████████| 130319/130319 [00:00<00:00, 1539811.04 examples/s]\n",
      "Generating validation split: 100%|██████████| 11873/11873 [00:00<00:00, 1878922.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SQuAD v2 dataset\n",
    "dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [example for example in train_dataset[:1000][\"context\"]]\n",
    "questions = [example for example in train_dataset[:1000][\"question\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get embeddings\n",
    "def embed_texts(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Embed the contexts\n",
    "context_embeddings = embed_texts(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 114180.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataset with required columns\n",
    "data_dict = {\n",
    "    \"title\": [\"\"] * len(contexts),  # Dummy titles as SQuAD does not provide titles\n",
    "    \"text\": contexts,\n",
    "    \"embeddings\": context_embeddings.tolist()  # Convert to list for serialization\n",
    "}\n",
    "new_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Save the new dataset to disk\n",
    "dataset_path = \"squad_v2_embeddings\"\n",
    "new_dataset.save_to_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Save the FAISS index to disk\n",
    "distance_type = \"Cosine\"\n",
    "index_path = \"faiss_index.faiss\"\n",
    "dimension = context_embeddings.shape[1]\n",
    "\n",
    "# Create the FAISS index\n",
    "if distance_type == \"L2\":\n",
    "    faiss_index = faiss.IndexFlatL2(dimension)\n",
    "elif distance_type == \"Cosine\":\n",
    "    faiss_index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "# Add the context embeddings to the index\n",
    "faiss_index.add(context_embeddings)\n",
    "faiss.write_index(faiss_index, index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, RagTokenizer, RagRetriever, RagSequenceForGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "generator_model_name = \"facebook/rag-token-nq\"\n",
    "rag_tokenizer = RagTokenizer.from_pretrained(generator_model_name)\n",
    "rag_model = RagSequenceForGeneration.from_pretrained(generator_model_name)\n",
    "# Load the dataset and index paths into the retriever\n",
    "rag_retriever = RagRetriever.from_pretrained(\n",
    "    generator_model_name,\n",
    "    index_name=\"custom\",\n",
    "    passages_path=dataset_path,\n",
    "    index_path=index_path,\n",
    "    use_dummy_dataset=False\n",
    ")\n",
    "# Associate the retriever with the RAG model\n",
    "rag_model.set_retriever(rag_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure retriever is set\n",
    "assert rag_model.retriever == rag_retriever, \"Retriever is not set correctly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate answers using RAG\n",
    "def generate_answer(question):\n",
    "    inputs = rag_tokenizer(question, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        result = rag_model.generate(input_ids=inputs[\"input_ids\"], num_return_sequences=1)\n",
    "    return rag_tokenizer.decode(result[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate answers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions[:\u001b[38;5;241m10\u001b[39m]:  \u001b[38;5;66;03m# Use a subset for demonstration\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[43], line 5\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m rag_tokenizer(question, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrag_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rag_tokenizer\u001b[38;5;241m.\u001b[39mdecode(result[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/transformers/models/rag/modeling_rag.py:984\u001b[0m, in \u001b[0;36mRagSequenceForGeneration.generate\u001b[0;34m(self, input_ids, attention_mask, context_input_ids, context_attention_mask, doc_scores, do_deduplication, num_return_sequences, num_beams, n_docs, **model_kwargs)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    983\u001b[0m     question_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestion_encoder(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 984\u001b[0m     context_input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;66;03m# set to correct device\u001b[39;00m\n\u001b[1;32m    993\u001b[0m     context_input_ids \u001b[38;5;241m=\u001b[39m context_input_ids\u001b[38;5;241m.\u001b[39mto(input_ids)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/transformers/models/rag/retrieval_rag.py:629\u001b[0m, in \u001b[0;36mRagRetriever.__call__\u001b[0;34m(self, question_input_ids, question_hidden_states, prefix, n_docs, return_tensors)\u001b[0m\n\u001b[1;32m    627\u001b[0m n_docs \u001b[38;5;241m=\u001b[39m n_docs \u001b[38;5;28;01mif\u001b[39;00m n_docs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_docs\n\u001b[1;32m    628\u001b[0m prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mprefix\n\u001b[0;32m--> 629\u001b[0m retrieved_doc_embeds, doc_ids, docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m input_strings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestion_encoder_tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(question_input_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    632\u001b[0m context_input_ids, context_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess_docs(\n\u001b[1;32m    633\u001b[0m     docs, input_strings, prefix, n_docs, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors\n\u001b[1;32m    634\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/transformers/models/rag/retrieval_rag.py:578\u001b[0m, in \u001b[0;36mRagRetriever.retrieve\u001b[0;34m(self, question_hidden_states, n_docs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, question_hidden_states: np\u001b[38;5;241m.\u001b[39mndarray, n_docs: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, List[\u001b[38;5;28mdict\u001b[39m]]:\n\u001b[1;32m    560\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;124;03m    Retrieves documents for specified `question_hidden_states`.\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m        - **doc_dicts** (`List[dict]`): The `retrieved_doc_embeds` examples per query.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m     doc_ids, retrieved_doc_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_doc_embeds, doc_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_doc_dicts(doc_ids)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/transformers/models/rag/retrieval_rag.py:548\u001b[0m, in \u001b[0;36mRagRetriever._main_retrieve\u001b[0;34m(self, question_hidden_states, n_docs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question_hidden_states \u001b[38;5;129;01min\u001b[39;00m question_hidden_states_batched:\n\u001b[1;32m    547\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 548\u001b[0m     ids, vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_top_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex search time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec, batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion_hidden_states\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m     ids_batched\u001b[38;5;241m.\u001b[39mextend(ids)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/transformers/models/rag/retrieval_rag.py:230\u001b[0m, in \u001b[0;36mHFIndexBase.get_top_docs\u001b[0;34m(self, question_hidden_states, n_docs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_top_docs\u001b[39m(\u001b[38;5;28mself\u001b[39m, question_hidden_states: np\u001b[38;5;241m.\u001b[39mndarray, n_docs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m--> 230\u001b[0m     _, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     docs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m indices \u001b[38;5;129;01min\u001b[39;00m ids]\n\u001b[1;32m    232\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m [doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/datasets/search.py:733\u001b[0m, in \u001b[0;36mIndexableMixin.search_batch\u001b[0;34m(self, index_name, queries, k, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find the nearest examples indices in the dataset to the query.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \n\u001b[1;32m    718\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;124;03m        - **total_indices** (`List[List[int]]`): the indices of the retrieved examples per query\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_index_is_initialized(index_name)\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indexes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/datasets/search.py:384\u001b[0m, in \u001b[0;36mFaissIndex.search_batch\u001b[0;34m(self, queries, k, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m queries\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous:\n\u001b[1;32m    383\u001b[0m     queries \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(queries, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 384\u001b[0m scores, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfaiss_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchedSearchResults(scores, indices\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_vs_crag/lib/python3.10/site-packages/faiss/class_wrappers.py:329\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[0;34m(self, x, k, params, D, I)\u001b[0m\n\u001b[1;32m    327\u001b[0m n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    328\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 329\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m D \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate answers\n",
    "for question in questions[:10]:  # Use a subset for demonstration\n",
    "    answer = generate_answer(question)\n",
    "    print(f\"Question: {question}\\nAnswer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RagChain\n",
    "from langchain.llms import HuggingFaceLLM\n",
    "from langchain.retrievers import FaissRetriever\n",
    "from langchain.tokenizers import HuggingFaceTokenizer\n",
    "\n",
    "# Create the retriever\n",
    "retriever = FaissRetriever(index=index, texts=contexts, embed_fn=embed_texts)\n",
    "\n",
    "# Create the LLM (generator)\n",
    "generator_model_name = \"facebook/rag-token-nq\"\n",
    "llm = HuggingFaceLLM(generator_model_name)\n",
    "\n",
    "# Create the tokenizer for the generator model\n",
    "tokenizer = HuggingFaceTokenizer(generator_model_name)\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = RagChain(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_vs_crag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
